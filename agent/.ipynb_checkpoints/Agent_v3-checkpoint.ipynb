{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Trading Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin_usd\n",
      "etherium_usd\n",
      "ripple_usd\n"
     ]
    }
   ],
   "source": [
    "#TO READ THE DATABASE\n",
    "import h5py\n",
    "\n",
    "DB_FILE = \"../data/dataset_1h_1000.hdf5\"\n",
    "DB = h5py.File(DB_FILE, \"r\")\n",
    "\n",
    "for item in DB:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.51543080e+12   1.47930000e+04   1.46940000e+04   1.48600000e+04\n",
      "    1.46690000e+04   7.86847895e+02]\n",
      " [  1.51542720e+12   1.44000000e+04   1.47890000e+04   1.49230000e+04\n",
      "    1.43470000e+04   4.20724394e+03]]\n"
     ]
    }
   ],
   "source": [
    "# Show the values of the first timestep\n",
    "print(DB[\"bitcoin_usd\"][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, frame_size, frame_parameter_number, aux_number, action_size):\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_parameter_number =  frame_parameter_number\n",
    "        self.aux_number = aux_number\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        # Main input (the time frame)\n",
    "        main_input = Input(shape=(frame_parameter_number,self.frame_size))\n",
    "        main = Flatten()(main_input)\n",
    "        \n",
    "        # Additional input (other inputs)\n",
    "        aux_input = Input(shape=(aux_number,))\n",
    "        \n",
    "        x = keras.layers.concatenate([main, aux_input])\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        out = Dense(self.action_size, activation='linear')(x)\n",
    "        \n",
    "        model = Model(inputs=[main_input, aux_input], outputs=out)\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # The agent acts randomly\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Predict the reward value based on the given state\n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        # Pick the action based on the predicted reward\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Extract informations from each memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            # if done, make our target reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # predict the future discounted reward\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "                \n",
    "            # make the agent to approximately map\n",
    "            # the current state to future discounted reward\n",
    "            # We'll call that target_f\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # Train the Neural Net with the state and target_f\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trading Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TradingGame:\n",
    "    def __init__(self, start_capital, frame_size, buy_step_size, database):\n",
    "        self.start_capital = start_capital\n",
    "        self.frame_size = frame_size\n",
    "        self.buy_step_size = buy_step_size\n",
    "        \n",
    "        self.database = np.asarray(database)\n",
    "        \n",
    "        self.capital = start_capital\n",
    "        self.liquid = start_capital\n",
    "        self.timestep = 0\n",
    "        self.time_frame = self.database[0:self.frame_size]\n",
    "        \n",
    "        self.position_price = 0.0\n",
    "        self.position_count = 0\n",
    "        self.state = None\n",
    "    \n",
    "    ### Reset function\n",
    "    def reset(self):\n",
    "        # Set everything to the initial start value\n",
    "        self.capital = self.start_capital\n",
    "        self.liquid = self.start_capital\n",
    "        self.timestep = 0\n",
    "        self.time_frame = self.database[0:self.frame_size]\n",
    "        \n",
    "        self.position_price = 0.0\n",
    "        self.position_count = 0\n",
    "        \n",
    "        # state => (time_frame, capital, position)\n",
    "        frame = np.reshape(self.time_frame, [1, 6, frame_size])\n",
    "        aux = np.reshape([self.liquid, self.capital, self.position_count, self.position_price], [1, 4])\n",
    "        self.state = [frame, aux]\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    ### Next frame function\n",
    "    def get_next_frame(self):\n",
    "        self.timestep += 1\n",
    "        \n",
    "        if (self.frame_size+self.timestep) < self.database.shape[0]:\n",
    "            next_frame = self.database[self.timestep:self.frame_size+self.timestep]\n",
    "            done = False\n",
    "        else:\n",
    "            next_frame = self.time_frame\n",
    "            done = True\n",
    "            \n",
    "        return next_frame, done\n",
    "    \n",
    "    def buy_positions(self, new_price, count):\n",
    "        # check if the position count is positive\n",
    "        if self.position_count >= 0:\n",
    "            # check if there is enough liquid money\n",
    "            if self.liquid > (count*new_price):\n",
    "                self.position_price = ((count*new_price)+(self.position_count*self.position_price))/(self.position_count+count)\n",
    "                self.position_count += count\n",
    "                self.liquid -= (count*new_price)\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = -100 * count\n",
    "                \n",
    "        elif (self.position_count <= -count):\n",
    "            self.position_count += count\n",
    "            self.liquid += (count*new_price)\n",
    "            reward = count*(self.position_price - new_price)\n",
    "            \n",
    "        else:\n",
    "            number_bought = self.position_count + count\n",
    "            number_sold = count - number_bought\n",
    "            \n",
    "            if self.liquid > ((number_bought*new_price)-(number_sold*new_price)):\n",
    "                self.position_price = new_price\n",
    "                self.position_count += count\n",
    "                self.liquid -= (number_bought*new_price)\n",
    "                self.liquid += (number_sold*new_price)\n",
    "                reward = number_sold*(self.position_price - new_price)\n",
    "            else:\n",
    "                reward = -100 * (number_bought-number_sold)\n",
    "        \n",
    "        if self.position_count == 0:\n",
    "            self.position_price = 0\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def sell_positions(self, new_price, count):\n",
    "        # check if the position count is negative\n",
    "        if self.position_count <= 0:\n",
    "            # check if there is enough liquid money\n",
    "            if self.liquid > (count*new_price):\n",
    "                self.position_price = ((count*new_price)+((-1)*self.position_count*self.position_price))/((-1)*self.position_count+count)\n",
    "                self.position_count -= count\n",
    "                self.liquid -= (count*new_price)\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = -100 * count\n",
    "                \n",
    "        elif (self.position_count >= count):\n",
    "            self.position_count -= count\n",
    "            self.liquid += (count*new_price)\n",
    "            reward = count*(new_price - self.position_price)\n",
    "            \n",
    "        else:\n",
    "            number_sold = count - self.position_count\n",
    "            number_bought = count - number_sold\n",
    "            \n",
    "            if self.liquid > ((number_bought*new_price)-(number_sold*new_price)):\n",
    "                self.position_price = new_price\n",
    "                self.position_count += count\n",
    "                self.liquid -= (number_bought*new_price)\n",
    "                self.liquid += (number_sold*new_price)\n",
    "                reward = number_sold*(self.position_price - new_price)\n",
    "            else:\n",
    "                reward = -100 * (number_bought-number_sold)\n",
    "        \n",
    "        if self.position_count == 0:\n",
    "            self.position_price = 0\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    ### Reward function\n",
    "    def calc_reward(self, action, next_frame):\n",
    "        # Get new price\n",
    "        new_price = next_frame[self.frame_size - 1, 2]\n",
    "        \n",
    "        # Check the action and calculate the reward\n",
    "        #hold\n",
    "        if action == 0: \n",
    "            reward = 0\n",
    "            \n",
    "        #buy one\n",
    "        elif action == 1: \n",
    "            reward = self.buy_positions(new_price, 1)\n",
    "                \n",
    "        #buy five\n",
    "        elif action == 2: \n",
    "            reward = self.buy_positions(new_price, self.buy_step_size)\n",
    "                \n",
    "        #sell one\n",
    "        elif action == 3: \n",
    "            reward = self.sell_positions(new_price, 1)\n",
    "                \n",
    "        #sell five        \n",
    "        elif action == 4: \n",
    "            reward = self.sell_positions(new_price, self.buy_step_size)\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    ### Next step function\n",
    "    def step(self, action):\n",
    "        # Get next time frame\n",
    "        next_frame, done = self.get_next_frame()\n",
    "        \n",
    "        # Get reward\n",
    "        reward = self.calc_reward(action, next_frame)\n",
    "        \n",
    "        # check if done\n",
    "        self.capital = self.liquid + self.position_count * self.position_price\n",
    "        if self.capital <= 0:\n",
    "            done = True\n",
    "            \n",
    "        # Next state\n",
    "        next_frame = np.reshape(next_frame, [1, 6, frame_size])\n",
    "        next_aux = np.reshape([self.liquid, self.capital, self.position_count, self.position_price], [1, 4])\n",
    "        next_state = [next_frame, next_aux]\n",
    "        \n",
    "        # update state\n",
    "        self.state = next_state\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter for Agent\n",
    "frame_size = 100 # Time frame\n",
    "frame_parameter_number = 6\n",
    "aux_number = 4 # Additional parameters\n",
    "action_size = 5 # hold, buy+1, buy+5, sell-1, sell-5\n",
    "buy_step_size = 3\n",
    "\n",
    "# Parameter for Trading Game\n",
    "database = DB[\"bitcoin_usd\"]\n",
    "batch_size = 64\n",
    "EPISODES = 1000\n",
    "CAPITAL = 100000\n",
    "#POSITION = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 6, 100)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 600)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 4)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 604)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 64)            38720                                        \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 64)            4160                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 32)            2080                                         \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 5)             165                                          \n",
      "====================================================================================================\n",
      "Total params: 45,125\n",
      "Trainable params: 45,125\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create agent\n",
    "agent = DQNAgent(frame_size, frame_parameter_number, aux_number, action_size)\n",
    "\n",
    "# Initialize Environment\n",
    "env = TradingGame(CAPITAL, frame_size, buy_step_size, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: -65096.88041056285, e: 1.0\n",
      "episode: 1/1000, score: -14942.691174700012, e: 0.99\n",
      "episode: 2/1000, score: -44966.0, e: 0.99\n",
      "episode: 3/1000, score: -16192.850539510022, e: 0.99\n",
      "episode: 4/1000, score: -53487.16044541332, e: 0.98\n",
      "episode: 5/1000, score: -13600.5723154, e: 0.98\n",
      "episode: 6/1000, score: -15907.049254429992, e: 0.97\n",
      "episode: 7/1000, score: -18378.21668828001, e: 0.97\n",
      "episode: 8/1000, score: 350911.48173313844, e: 0.96\n",
      "episode: 9/1000, score: 490035.41448619775, e: 0.96\n",
      "episode: 10/1000, score: 397360.15790782927, e: 0.95\n",
      "episode: 11/1000, score: -17693.355627120007, e: 0.95\n",
      "episode: 12/1000, score: -54190.706513059995, e: 0.94\n",
      "episode: 13/1000, score: -31244.867226443253, e: 0.94\n",
      "episode: 14/1000, score: -21920.939142632007, e: 0.93\n",
      "episode: 15/1000, score: -71846.0, e: 0.93\n",
      "episode: 16/1000, score: -15938.0, e: 0.92\n",
      "episode: 17/1000, score: 349053.99953820015, e: 0.92\n",
      "episode: 18/1000, score: -76498.54409129001, e: 0.91\n",
      "episode: 19/1000, score: -50796.40972185001, e: 0.91\n",
      "episode: 20/1000, score: -19358.848446310032, e: 0.9\n",
      "episode: 21/1000, score: 316422.36650957534, e: 0.9\n",
      "episode: 22/1000, score: -78349.86106116, e: 0.9\n",
      "episode: 23/1000, score: -18915.657258630003, e: 0.89\n",
      "episode: 24/1000, score: 283645.9657211599, e: 0.89\n",
      "episode: 25/1000, score: -18028.39673698001, e: 0.88\n",
      "episode: 26/1000, score: 215972.02025573305, e: 0.88\n",
      "episode: 27/1000, score: 407813.89530670317, e: 0.87\n",
      "episode: 28/1000, score: 270142.88704127003, e: 0.87\n",
      "episode: 29/1000, score: -19391.9113371, e: 0.86\n",
      "episode: 30/1000, score: 417986.388203265, e: 0.86\n",
      "episode: 31/1000, score: -19891.520228154448, e: 0.86\n",
      "episode: 32/1000, score: 235879.61782964744, e: 0.85\n",
      "episode: 33/1000, score: 333196.98327907955, e: 0.85\n",
      "episode: 34/1000, score: -5349.765892905998, e: 0.84\n",
      "episode: 35/1000, score: 209313.60537632334, e: 0.84\n",
      "episode: 36/1000, score: 326509.90059394686, e: 0.83\n",
      "episode: 37/1000, score: 501662.39736289764, e: 0.83\n",
      "episode: 38/1000, score: -11704.056719119995, e: 0.83\n",
      "episode: 39/1000, score: 227998.88546550536, e: 0.82\n",
      "episode: 40/1000, score: -18191.670144380012, e: 0.82\n",
      "episode: 41/1000, score: -60646.35788421001, e: 0.81\n",
      "episode: 42/1000, score: 428414.2378396065, e: 0.81\n",
      "episode: 43/1000, score: 225942.95921651172, e: 0.81\n",
      "episode: 44/1000, score: -19483.0, e: 0.8\n",
      "episode: 45/1000, score: -44966.0, e: 0.8\n",
      "episode: 46/1000, score: 214458.45811037696, e: 0.79\n",
      "episode: 47/1000, score: 182501.2773436086, e: 0.79\n",
      "episode: 48/1000, score: -44405.503763620014, e: 0.79\n",
      "episode: 49/1000, score: 317616.63646403124, e: 0.78\n",
      "episode: 50/1000, score: 316274.18203602615, e: 0.78\n",
      "episode: 51/1000, score: 166064.35951154408, e: 0.77\n",
      "episode: 52/1000, score: -69705.10427216001, e: 0.77\n",
      "episode: 53/1000, score: -36250.943280880005, e: 0.77\n",
      "episode: 54/1000, score: -77851.0, e: 0.76\n",
      "episode: 55/1000, score: -15254.0, e: 0.76\n",
      "episode: 56/1000, score: 199193.37062151748, e: 0.76\n",
      "episode: 57/1000, score: -78089.51637450006, e: 0.75\n",
      "episode: 58/1000, score: -17846.18786631002, e: 0.75\n",
      "episode: 59/1000, score: -45479.0, e: 0.74\n",
      "episode: 60/1000, score: -14742.699465880025, e: 0.74\n",
      "episode: 61/1000, score: -1662.7965132800164, e: 0.74\n",
      "episode: 62/1000, score: 264453.38686661096, e: 0.73\n",
      "episode: 63/1000, score: 266404.5358947252, e: 0.73\n",
      "episode: 64/1000, score: 332280.1880937867, e: 0.73\n",
      "episode: 65/1000, score: 201529.0767566276, e: 0.72\n",
      "episode: 66/1000, score: 353875.33631390147, e: 0.72\n",
      "episode: 67/1000, score: -18779.76229119001, e: 0.71\n",
      "episode: 68/1000, score: -15938.0, e: 0.71\n",
      "episode: 69/1000, score: -89174.51187559002, e: 0.71\n",
      "episode: 70/1000, score: -15938.0, e: 0.7\n",
      "episode: 71/1000, score: -10421.032021360035, e: 0.7\n",
      "episode: 72/1000, score: 199740.22974217913, e: 0.7\n",
      "episode: 73/1000, score: 221194.98071647715, e: 0.69\n",
      "episode: 74/1000, score: -15254.0, e: 0.69\n",
      "episode: 75/1000, score: -46895.94328087999, e: 0.69\n",
      "episode: 76/1000, score: 209055.76901215626, e: 0.68\n",
      "episode: 77/1000, score: -39813.04026614998, e: 0.68\n",
      "episode: 78/1000, score: -19437.491842449992, e: 0.68\n",
      "episode: 79/1000, score: 120867.04218004523, e: 0.67\n",
      "episode: 80/1000, score: -26588.41930619, e: 0.67\n",
      "episode: 81/1000, score: -15938.0, e: 0.67\n",
      "episode: 82/1000, score: 214778.88360309467, e: 0.66\n",
      "episode: 83/1000, score: -27551.680776240013, e: 0.66\n",
      "episode: 84/1000, score: 252857.24819462816, e: 0.66\n",
      "episode: 85/1000, score: -73394.0, e: 0.65\n",
      "episode: 86/1000, score: 158611.79026631522, e: 0.65\n",
      "episode: 87/1000, score: 157234.363464827, e: 0.65\n",
      "episode: 88/1000, score: -33330.99263472998, e: 0.64\n",
      "episode: 89/1000, score: -21736.351756310003, e: 0.64\n",
      "episode: 90/1000, score: -73394.0, e: 0.64\n",
      "episode: 91/1000, score: 156359.4957157276, e: 0.63\n",
      "episode: 92/1000, score: 185637.58119574183, e: 0.63\n",
      "episode: 93/1000, score: 173655.7423327819, e: 0.63\n",
      "episode: 94/1000, score: 250792.88575621057, e: 0.62\n",
      "episode: 95/1000, score: 193395.56087141493, e: 0.62\n",
      "episode: 96/1000, score: -15858.0, e: 0.62\n",
      "episode: 97/1000, score: -44751.0, e: 0.61\n",
      "episode: 98/1000, score: -7650.082511259985, e: 0.61\n",
      "episode: 99/1000, score: -4086.4131741599995, e: 0.61\n",
      "episode: 100/1000, score: 211076.26919224756, e: 0.61\n",
      "episode: 101/1000, score: -74285.0, e: 0.6\n",
      "episode: 102/1000, score: -15254.0, e: 0.6\n",
      "episode: 103/1000, score: -73394.0, e: 0.6\n",
      "episode: 104/1000, score: 220287.9382420841, e: 0.59\n",
      "episode: 105/1000, score: 233282.92675592974, e: 0.59\n",
      "episode: 106/1000, score: 170568.4157568349, e: 0.59\n",
      "episode: 107/1000, score: 173002.00652476883, e: 0.58\n",
      "episode: 108/1000, score: 130953.10241731252, e: 0.58\n",
      "episode: 109/1000, score: 179679.40615545434, e: 0.58\n",
      "episode: 110/1000, score: 184059.82010526088, e: 0.58\n",
      "episode: 111/1000, score: 184756.9409893131, e: 0.57\n",
      "episode: 112/1000, score: 187248.75684319995, e: 0.57\n",
      "episode: 113/1000, score: 139540.32439981407, e: 0.57\n",
      "episode: 114/1000, score: -73943.0, e: 0.56\n",
      "episode: 115/1000, score: 130355.26955703512, e: 0.56\n",
      "episode: 116/1000, score: -15052.0, e: 0.56\n",
      "episode: 117/1000, score: 152988.36250461574, e: 0.56\n",
      "episode: 118/1000, score: -18510.324421580008, e: 0.55\n",
      "episode: 119/1000, score: -15938.0, e: 0.55\n",
      "episode: 120/1000, score: 143551.08420217197, e: 0.55\n",
      "episode: 121/1000, score: 155300.82736336146, e: 0.55\n",
      "episode: 122/1000, score: 157940.27601772046, e: 0.54\n",
      "episode: 123/1000, score: 199213.63873266673, e: 0.54\n",
      "episode: 124/1000, score: 180459.244975377, e: 0.54\n",
      "episode: 125/1000, score: 145685.30881871664, e: 0.53\n",
      "episode: 126/1000, score: 78412.41454721002, e: 0.53\n",
      "episode: 127/1000, score: 143772.82975999467, e: 0.53\n",
      "episode: 128/1000, score: 158211.09561225568, e: 0.53\n",
      "episode: 129/1000, score: 168669.4501577465, e: 0.52\n",
      "episode: 130/1000, score: 149645.3507940012, e: 0.52\n",
      "episode: 131/1000, score: 152758.34998749697, e: 0.52\n",
      "episode: 132/1000, score: 124832.45036535915, e: 0.52\n",
      "episode: 133/1000, score: -12675.0, e: 0.51\n",
      "episode: 134/1000, score: 147707.35036889426, e: 0.51\n",
      "episode: 135/1000, score: 230094.17536881042, e: 0.51\n",
      "episode: 136/1000, score: 153617.66398614293, e: 0.51\n",
      "episode: 137/1000, score: 142422.0749747845, e: 0.5\n",
      "episode: 138/1000, score: 139179.6862649633, e: 0.5\n",
      "episode: 139/1000, score: 144652.08752092588, e: 0.5\n",
      "episode: 140/1000, score: 131510.14505709056, e: 0.5\n",
      "episode: 141/1000, score: 107523.80554589207, e: 0.49\n",
      "episode: 142/1000, score: 167938.63829366726, e: 0.49\n",
      "episode: 143/1000, score: 112860.33984631058, e: 0.49\n",
      "episode: 144/1000, score: 121875.80321665588, e: 0.49\n",
      "episode: 145/1000, score: 112429.8782367961, e: 0.48\n",
      "episode: 146/1000, score: 165050.04874321455, e: 0.48\n",
      "episode: 147/1000, score: -21309.698368490004, e: 0.48\n",
      "episode: 148/1000, score: 109978.81845855698, e: 0.48\n",
      "episode: 149/1000, score: 120252.43499323326, e: 0.47\n",
      "episode: 150/1000, score: 149468.28275101192, e: 0.47\n",
      "episode: 151/1000, score: 108552.70288788935, e: 0.47\n",
      "episode: 152/1000, score: 139271.75777423766, e: 0.47\n",
      "episode: 153/1000, score: 138547.90206884264, e: 0.46\n",
      "episode: 154/1000, score: -15485.0, e: 0.46\n",
      "episode: 155/1000, score: 99604.88059346657, e: 0.46\n",
      "episode: 156/1000, score: 125496.27394944798, e: 0.46\n",
      "episode: 157/1000, score: 156440.40268844063, e: 0.46\n",
      "episode: 158/1000, score: 108152.5250163574, e: 0.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 159/1000, score: 133046.33411364135, e: 0.45\n",
      "episode: 160/1000, score: 140887.99855607998, e: 0.45\n",
      "episode: 161/1000, score: 109674.06919632078, e: 0.45\n",
      "episode: 162/1000, score: -20247.283019399998, e: 0.44\n",
      "episode: 163/1000, score: 91184.81415676, e: 0.44\n",
      "episode: 164/1000, score: 114322.74201688948, e: 0.44\n",
      "episode: 165/1000, score: 84049.5227739746, e: 0.44\n",
      "episode: 166/1000, score: -45479.0, e: 0.44\n",
      "episode: 167/1000, score: 114267.85347606253, e: 0.43\n",
      "episode: 168/1000, score: 107514.34047763274, e: 0.43\n",
      "episode: 169/1000, score: 178895.41401095194, e: 0.43\n",
      "episode: 170/1000, score: 104519.39687399109, e: 0.43\n",
      "episode: 171/1000, score: 114498.89656072749, e: 0.42\n",
      "episode: 172/1000, score: 95719.52831522629, e: 0.42\n",
      "episode: 173/1000, score: 151909.95062760243, e: 0.42\n",
      "episode: 174/1000, score: 146250.7113504796, e: 0.42\n",
      "episode: 175/1000, score: 101290.41105928094, e: 0.42\n",
      "episode: 176/1000, score: 134652.70098934334, e: 0.41\n",
      "episode: 177/1000, score: 154495.25391182504, e: 0.41\n",
      "episode: 178/1000, score: 91499.50968190648, e: 0.41\n",
      "episode: 179/1000, score: 147129.80554112146, e: 0.41\n",
      "episode: 180/1000, score: -953.5966082699888, e: 0.41\n",
      "episode: 181/1000, score: 83071.68957059199, e: 0.4\n",
      "episode: 182/1000, score: 139136.33085141456, e: 0.4\n",
      "episode: 183/1000, score: 100825.19793947568, e: 0.4\n",
      "episode: 184/1000, score: 98153.59085070032, e: 0.4\n",
      "episode: 185/1000, score: 108382.95834250105, e: 0.4\n",
      "episode: 186/1000, score: 136247.67649258443, e: 0.39\n",
      "episode: 187/1000, score: 115427.47799636384, e: 0.39\n",
      "episode: 188/1000, score: -48977.04925442999, e: 0.39\n",
      "episode: 189/1000, score: 121301.42647018179, e: 0.39\n",
      "episode: 190/1000, score: 103827.42779490704, e: 0.39\n",
      "episode: 191/1000, score: 98787.94809267281, e: 0.38\n",
      "episode: 192/1000, score: 123895.90265245471, e: 0.38\n",
      "episode: 193/1000, score: 94337.9214699402, e: 0.38\n",
      "episode: 194/1000, score: 126621.19962212731, e: 0.38\n",
      "episode: 195/1000, score: 136150.3882868004, e: 0.38\n",
      "episode: 196/1000, score: 121160.84036511603, e: 0.37\n",
      "episode: 197/1000, score: 131195.78934482473, e: 0.37\n",
      "episode: 198/1000, score: 96591.69613397178, e: 0.37\n",
      "episode: 199/1000, score: 110889.23292579372, e: 0.37\n",
      "episode: 200/1000, score: 125398.65361087072, e: 0.37\n",
      "episode: 201/1000, score: 117481.72464962896, e: 0.37\n",
      "episode: 202/1000, score: 132042.30627436796, e: 0.36\n",
      "episode: 203/1000, score: 97932.6454112691, e: 0.36\n",
      "episode: 204/1000, score: 76171.21541162085, e: 0.36\n",
      "episode: 205/1000, score: 116060.45922901428, e: 0.36\n",
      "episode: 206/1000, score: 116756.95103908249, e: 0.36\n",
      "episode: 207/1000, score: 145290.8897311673, e: 0.35\n",
      "episode: 208/1000, score: 102348.902479961, e: 0.35\n",
      "episode: 209/1000, score: 108741.35561705848, e: 0.35\n",
      "episode: 210/1000, score: 95182.32427302185, e: 0.35\n",
      "episode: 211/1000, score: 124124.97872030645, e: 0.35\n",
      "episode: 212/1000, score: 94941.27810615477, e: 0.35\n",
      "episode: 213/1000, score: 114210.21894217904, e: 0.34\n",
      "episode: 214/1000, score: 119893.60932165654, e: 0.34\n",
      "episode: 215/1000, score: 123847.33115707946, e: 0.34\n",
      "episode: 216/1000, score: 166465.98439032133, e: 0.34\n",
      "episode: 217/1000, score: 114184.9203259643, e: 0.34\n",
      "episode: 218/1000, score: 124402.19853584527, e: 0.34\n",
      "episode: 219/1000, score: 85793.065062835, e: 0.33\n",
      "episode: 220/1000, score: 137109.5870088412, e: 0.33\n",
      "episode: 221/1000, score: 121363.63818862909, e: 0.33\n",
      "episode: 222/1000, score: 119216.44180727281, e: 0.33\n",
      "episode: 223/1000, score: 118275.8128468988, e: 0.33\n",
      "episode: 224/1000, score: 87728.12725384494, e: 0.33\n",
      "episode: 225/1000, score: 113319.98824501547, e: 0.32\n",
      "episode: 226/1000, score: 94975.50475091461, e: 0.32\n",
      "episode: 227/1000, score: 114390.14131239399, e: 0.32\n",
      "episode: 228/1000, score: 103372.66388805442, e: 0.32\n",
      "episode: 229/1000, score: 105450.6155791379, e: 0.32\n",
      "episode: 230/1000, score: 118453.80832190068, e: 0.32\n",
      "episode: 231/1000, score: 127868.06931203704, e: 0.31\n",
      "episode: 232/1000, score: 92099.65574385514, e: 0.31\n",
      "episode: 233/1000, score: 92019.5568332092, e: 0.31\n",
      "episode: 234/1000, score: 107154.30440444656, e: 0.31\n",
      "episode: 235/1000, score: 102865.18920832836, e: 0.31\n",
      "episode: 236/1000, score: 102411.89588922422, e: 0.31\n",
      "episode: 237/1000, score: 103486.58797398362, e: 0.3\n",
      "episode: 238/1000, score: 169429.11297802467, e: 0.3\n",
      "episode: 239/1000, score: 121688.84568497581, e: 0.3\n",
      "episode: 240/1000, score: 107161.4020074549, e: 0.3\n",
      "episode: 241/1000, score: 69673.87466126285, e: 0.3\n",
      "episode: 242/1000, score: 81432.5048578236, e: 0.3\n",
      "episode: 243/1000, score: 129187.69824677308, e: 0.3\n",
      "episode: 244/1000, score: 105787.9903391107, e: 0.29\n",
      "episode: 245/1000, score: 76374.79627072638, e: 0.29\n",
      "episode: 246/1000, score: 120247.6657794169, e: 0.29\n",
      "episode: 247/1000, score: 95067.54359231042, e: 0.29\n",
      "episode: 248/1000, score: 60585.89282556856, e: 0.29\n",
      "episode: 249/1000, score: 78612.81024945361, e: 0.29\n",
      "episode: 250/1000, score: 116559.34404838822, e: 0.29\n",
      "episode: 251/1000, score: -15233.0, e: 0.28\n",
      "episode: 252/1000, score: 84477.21107820442, e: 0.28\n",
      "episode: 253/1000, score: 100212.38121661411, e: 0.28\n",
      "episode: 254/1000, score: 72014.98773627273, e: 0.28\n",
      "episode: 255/1000, score: 86007.83258189916, e: 0.28\n",
      "episode: 256/1000, score: 90211.40827525695, e: 0.28\n",
      "episode: 257/1000, score: 96095.73222265353, e: 0.28\n",
      "episode: 258/1000, score: 108631.7998530971, e: 0.27\n",
      "episode: 259/1000, score: 147165.19986445695, e: 0.27\n",
      "episode: 260/1000, score: 111078.909049606, e: 0.27\n",
      "episode: 261/1000, score: 81646.67179581252, e: 0.27\n",
      "episode: 262/1000, score: 117116.50129497, e: 0.27\n",
      "episode: 263/1000, score: 132979.9186240344, e: 0.27\n",
      "episode: 264/1000, score: 106154.62172933681, e: 0.27\n",
      "episode: 265/1000, score: 118266.36977869664, e: 0.26\n",
      "episode: 266/1000, score: 76724.98945646515, e: 0.26\n",
      "episode: 267/1000, score: 105216.23869769863, e: 0.26\n",
      "episode: 268/1000, score: 116735.68391544421, e: 0.26\n",
      "episode: 269/1000, score: 112139.32135313292, e: 0.26\n",
      "episode: 270/1000, score: 72867.59185823967, e: 0.26\n",
      "episode: 271/1000, score: 87134.5490742687, e: 0.26\n",
      "episode: 272/1000, score: 138588.85090443853, e: 0.26\n",
      "episode: 273/1000, score: 69806.81465446268, e: 0.25\n",
      "episode: 274/1000, score: 76116.06756350615, e: 0.25\n",
      "episode: 275/1000, score: 102391.9504796522, e: 0.25\n",
      "episode: 276/1000, score: 90183.47960601805, e: 0.25\n",
      "episode: 277/1000, score: 74444.31871253825, e: 0.25\n",
      "episode: 278/1000, score: 99885.25916044047, e: 0.25\n",
      "episode: 279/1000, score: 96462.28893520635, e: 0.25\n",
      "episode: 280/1000, score: 80814.33711789001, e: 0.25\n",
      "episode: 281/1000, score: 100073.95655633665, e: 0.24\n",
      "episode: 282/1000, score: 107824.6164185161, e: 0.24\n",
      "episode: 283/1000, score: 84220.44939823545, e: 0.24\n",
      "episode: 284/1000, score: 71039.60821062364, e: 0.24\n",
      "episode: 285/1000, score: 76223.70593969892, e: 0.24\n",
      "episode: 286/1000, score: 83221.91278020816, e: 0.24\n",
      "episode: 287/1000, score: 69861.36604914875, e: 0.24\n",
      "episode: 288/1000, score: 76416.23025187926, e: 0.24\n",
      "episode: 289/1000, score: 82791.04020628084, e: 0.23\n",
      "episode: 290/1000, score: 101851.71787873298, e: 0.23\n",
      "episode: 291/1000, score: 113756.47201868646, e: 0.23\n",
      "episode: 292/1000, score: 75862.4925083416, e: 0.23\n",
      "episode: 293/1000, score: 76377.45119897298, e: 0.23\n",
      "episode: 294/1000, score: 83880.63281784015, e: 0.23\n",
      "episode: 295/1000, score: 128718.73521532238, e: 0.23\n",
      "episode: 296/1000, score: 71283.03324619344, e: 0.23\n",
      "episode: 297/1000, score: 92630.67280373086, e: 0.23\n",
      "episode: 298/1000, score: 82736.40385238975, e: 0.22\n",
      "episode: 299/1000, score: 76195.72495106033, e: 0.22\n",
      "episode: 300/1000, score: 73673.28444342157, e: 0.22\n",
      "episode: 301/1000, score: 102731.80851023365, e: 0.22\n",
      "episode: 302/1000, score: 73447.22690400899, e: 0.22\n",
      "episode: 303/1000, score: 72301.79675815967, e: 0.22\n",
      "episode: 304/1000, score: 74424.5642928128, e: 0.22\n",
      "episode: 305/1000, score: 125303.68242366018, e: 0.22\n",
      "episode: 306/1000, score: 89398.48782884928, e: 0.22\n",
      "episode: 307/1000, score: 72203.77485931013, e: 0.21\n",
      "episode: 308/1000, score: 78633.07498780692, e: 0.21\n",
      "episode: 309/1000, score: 124210.77484847292, e: 0.21\n",
      "episode: 310/1000, score: 74885.00606538352, e: 0.21\n",
      "episode: 311/1000, score: 70841.17479884138, e: 0.21\n",
      "episode: 312/1000, score: 83884.60588678585, e: 0.21\n",
      "episode: 313/1000, score: 85555.51417316377, e: 0.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 314/1000, score: 75567.27843909556, e: 0.21\n",
      "episode: 315/1000, score: 127587.31076671128, e: 0.21\n",
      "episode: 316/1000, score: 118119.72119518406, e: 0.21\n",
      "episode: 317/1000, score: 81028.34872677477, e: 0.2\n",
      "episode: 318/1000, score: 103704.25569777413, e: 0.2\n",
      "episode: 319/1000, score: 93095.83044746147, e: 0.2\n",
      "episode: 320/1000, score: 76565.99713461801, e: 0.2\n",
      "episode: 321/1000, score: 92761.64565635851, e: 0.2\n",
      "episode: 322/1000, score: 74282.71567490722, e: 0.2\n",
      "episode: 323/1000, score: 73640.16614920883, e: 0.2\n",
      "episode: 324/1000, score: 79919.0886538135, e: 0.2\n",
      "episode: 325/1000, score: 72782.55125761149, e: 0.2\n",
      "episode: 326/1000, score: 99946.0585535941, e: 0.2\n",
      "episode: 327/1000, score: 73113.26438757342, e: 0.19\n",
      "episode: 328/1000, score: 78505.45966998779, e: 0.19\n",
      "episode: 329/1000, score: 77222.25519118199, e: 0.19\n",
      "episode: 330/1000, score: 75193.30929475595, e: 0.19\n",
      "episode: 331/1000, score: 122703.62302283282, e: 0.19\n",
      "episode: 332/1000, score: 122866.71098789474, e: 0.19\n",
      "episode: 333/1000, score: 81128.66462929647, e: 0.19\n",
      "episode: 334/1000, score: 93566.67114250187, e: 0.19\n",
      "episode: 335/1000, score: 93273.70700983639, e: 0.19\n",
      "episode: 336/1000, score: 69131.39974409789, e: 0.19\n",
      "episode: 337/1000, score: 77573.65754304183, e: 0.18\n",
      "episode: 338/1000, score: 78768.51277219718, e: 0.18\n",
      "episode: 339/1000, score: 63159.44369246899, e: 0.18\n",
      "episode: 340/1000, score: 77182.60430013435, e: 0.18\n",
      "episode: 341/1000, score: 84193.70252974101, e: 0.18\n",
      "episode: 342/1000, score: 74212.12693225728, e: 0.18\n",
      "episode: 343/1000, score: 80108.99449134668, e: 0.18\n",
      "episode: 344/1000, score: 74766.54090048163, e: 0.18\n",
      "episode: 345/1000, score: 63900.82825316832, e: 0.18\n",
      "episode: 346/1000, score: 94626.79591635845, e: 0.18\n",
      "episode: 347/1000, score: 82345.74602466714, e: 0.18\n",
      "episode: 348/1000, score: 73396.52778111219, e: 0.17\n",
      "episode: 349/1000, score: 70779.5805707006, e: 0.17\n",
      "episode: 350/1000, score: 75092.30495179599, e: 0.17\n",
      "episode: 351/1000, score: 80910.6734079928, e: 0.17\n",
      "episode: 352/1000, score: 73871.69807836799, e: 0.17\n",
      "episode: 353/1000, score: 105301.01379625639, e: 0.17\n",
      "episode: 354/1000, score: 86891.95049660646, e: 0.17\n",
      "episode: 355/1000, score: 69774.69247773755, e: 0.17\n",
      "episode: 356/1000, score: 82328.59086968635, e: 0.17\n",
      "episode: 357/1000, score: 70247.64855132019, e: 0.17\n",
      "episode: 358/1000, score: 85057.65096166736, e: 0.17\n",
      "episode: 359/1000, score: 74026.75066532972, e: 0.17\n",
      "episode: 360/1000, score: 78848.24281059313, e: 0.16\n",
      "episode: 361/1000, score: 92999.3423127103, e: 0.16\n",
      "episode: 362/1000, score: 69877.83826799062, e: 0.16\n",
      "episode: 363/1000, score: 71226.10709077034, e: 0.16\n",
      "episode: 364/1000, score: 75359.46536162727, e: 0.16\n",
      "episode: 365/1000, score: 75701.06638622806, e: 0.16\n",
      "episode: 366/1000, score: 75156.2654973979, e: 0.16\n",
      "episode: 367/1000, score: 73614.48473094817, e: 0.16\n",
      "episode: 368/1000, score: 84797.28282637788, e: 0.16\n",
      "episode: 369/1000, score: 82072.56876833372, e: 0.16\n",
      "episode: 370/1000, score: 76752.49479718023, e: 0.16\n",
      "episode: 371/1000, score: 78532.37990117213, e: 0.16\n",
      "episode: 372/1000, score: 80388.3450204386, e: 0.15\n",
      "episode: 373/1000, score: 82788.72266863385, e: 0.15\n",
      "episode: 374/1000, score: 69955.24208034974, e: 0.15\n",
      "episode: 375/1000, score: 71196.55119528092, e: 0.15\n",
      "episode: 376/1000, score: 86276.34149215213, e: 0.15\n",
      "episode: 377/1000, score: 73838.94584641814, e: 0.15\n",
      "episode: 378/1000, score: 74503.04747280153, e: 0.15\n",
      "episode: 379/1000, score: 88037.60269587913, e: 0.15\n",
      "episode: 380/1000, score: 80464.60637457934, e: 0.15\n",
      "episode: 381/1000, score: 79342.6120267043, e: 0.15\n",
      "episode: 382/1000, score: 81871.00000323605, e: 0.15\n",
      "episode: 383/1000, score: 82525.71975311979, e: 0.15\n",
      "episode: 384/1000, score: 74621.73121848262, e: 0.15\n",
      "episode: 385/1000, score: 75032.21045555263, e: 0.15\n",
      "episode: 386/1000, score: 98390.5319088544, e: 0.14\n",
      "episode: 387/1000, score: 72607.19207344396, e: 0.14\n",
      "episode: 388/1000, score: 93848.66466457154, e: 0.14\n",
      "episode: 389/1000, score: 79621.8936950988, e: 0.14\n",
      "episode: 390/1000, score: 73029.77310468159, e: 0.14\n",
      "episode: 391/1000, score: 73526.68950334459, e: 0.14\n",
      "episode: 392/1000, score: 74806.71045947759, e: 0.14\n",
      "episode: 393/1000, score: 81834.0168767829, e: 0.14\n",
      "episode: 394/1000, score: 74257.73376598238, e: 0.14\n",
      "episode: 395/1000, score: 76600.18275498634, e: 0.14\n",
      "episode: 396/1000, score: 76377.6328012916, e: 0.14\n",
      "episode: 397/1000, score: 72127.10630048, e: 0.14\n",
      "episode: 398/1000, score: 73679.86317556715, e: 0.14\n",
      "episode: 399/1000, score: 76167.6179336136, e: 0.14\n",
      "episode: 400/1000, score: 90007.23027318102, e: 0.13\n",
      "episode: 401/1000, score: 80860.73759490394, e: 0.13\n",
      "episode: 402/1000, score: 77468.03557628565, e: 0.13\n",
      "episode: 403/1000, score: 79310.17370513537, e: 0.13\n",
      "episode: 404/1000, score: 75158.77230967762, e: 0.13\n",
      "episode: 405/1000, score: 68612.4110249837, e: 0.13\n",
      "episode: 406/1000, score: 70814.27172932563, e: 0.13\n",
      "episode: 407/1000, score: 74270.3934318362, e: 0.13\n",
      "episode: 408/1000, score: 76387.17974003904, e: 0.13\n",
      "episode: 409/1000, score: 92539.70434922018, e: 0.13\n",
      "episode: 410/1000, score: 78448.81198451301, e: 0.13\n",
      "episode: 411/1000, score: 80905.60812680205, e: 0.13\n",
      "episode: 412/1000, score: 83226.30608500025, e: 0.13\n",
      "episode: 413/1000, score: 71071.86752234577, e: 0.13\n",
      "episode: 414/1000, score: 78258.33216058197, e: 0.13\n",
      "episode: 415/1000, score: 78483.91985605663, e: 0.12\n",
      "episode: 416/1000, score: 73507.81974238291, e: 0.12\n",
      "episode: 417/1000, score: 78784.6490239205, e: 0.12\n",
      "episode: 418/1000, score: 82064.1279424888, e: 0.12\n",
      "episode: 419/1000, score: 76712.7185367611, e: 0.12\n",
      "episode: 420/1000, score: 88943.38936592468, e: 0.12\n",
      "episode: 421/1000, score: 71953.57387870498, e: 0.12\n",
      "episode: 422/1000, score: 73035.62481871173, e: 0.12\n",
      "episode: 423/1000, score: 81760.91380717797, e: 0.12\n",
      "episode: 424/1000, score: 120328.20538182449, e: 0.12\n",
      "episode: 425/1000, score: 84244.58711865218, e: 0.12\n",
      "episode: 426/1000, score: 83963.02840916491, e: 0.12\n",
      "episode: 427/1000, score: 113360.9797284744, e: 0.12\n",
      "episode: 428/1000, score: 82458.74586395804, e: 0.12\n",
      "episode: 429/1000, score: 117435.94693122728, e: 0.12\n",
      "episode: 430/1000, score: 76531.00519074516, e: 0.12\n",
      "episode: 431/1000, score: 72023.93862008522, e: 0.12\n",
      "episode: 432/1000, score: 75236.82111433618, e: 0.11\n",
      "episode: 433/1000, score: 78607.37206432565, e: 0.11\n",
      "episode: 434/1000, score: 74329.91555555967, e: 0.11\n",
      "episode: 435/1000, score: 81273.45170205136, e: 0.11\n",
      "episode: 436/1000, score: 83410.58218260249, e: 0.11\n",
      "episode: 437/1000, score: 73495.49611331474, e: 0.11\n",
      "episode: 438/1000, score: 80260.29356247334, e: 0.11\n",
      "episode: 439/1000, score: 71272.34517581188, e: 0.11\n",
      "episode: 440/1000, score: 75325.71133365523, e: 0.11\n",
      "episode: 441/1000, score: 73822.64723198087, e: 0.11\n",
      "episode: 442/1000, score: 80840.967643123, e: 0.11\n",
      "episode: 443/1000, score: 80046.90062808743, e: 0.11\n",
      "episode: 444/1000, score: 83612.46617038456, e: 0.11\n",
      "episode: 445/1000, score: 80694.10494188024, e: 0.11\n",
      "episode: 446/1000, score: 76264.35538658462, e: 0.11\n",
      "episode: 447/1000, score: 76895.8709513596, e: 0.11\n",
      "episode: 448/1000, score: 81776.09836744476, e: 0.11\n",
      "episode: 449/1000, score: 81716.01868770976, e: 0.11\n",
      "episode: 450/1000, score: 85448.71726632374, e: 0.1\n",
      "episode: 451/1000, score: 79200.36596579252, e: 0.1\n",
      "episode: 452/1000, score: 78557.82936345931, e: 0.1\n",
      "episode: 453/1000, score: 87412.22878192665, e: 0.1\n",
      "episode: 454/1000, score: 92439.36265817407, e: 0.1\n",
      "episode: 455/1000, score: 76669.04890064023, e: 0.1\n",
      "episode: 456/1000, score: 84874.52572535486, e: 0.1\n",
      "episode: 457/1000, score: 81388.55823470015, e: 0.1\n",
      "episode: 458/1000, score: 65548.94512479377, e: 0.1\n",
      "episode: 459/1000, score: 75703.7758044966, e: 0.1\n",
      "episode: 460/1000, score: 77535.68597766625, e: 0.1\n",
      "episode: 461/1000, score: 77598.40786471675, e: 0.099\n",
      "episode: 462/1000, score: 89709.02240352162, e: 0.099\n",
      "episode: 463/1000, score: 97278.26037016466, e: 0.098\n",
      "episode: 464/1000, score: 77883.89318399278, e: 0.098\n",
      "episode: 465/1000, score: 83427.66009228396, e: 0.097\n",
      "episode: 466/1000, score: 75366.261924063, e: 0.097\n",
      "episode: 467/1000, score: 86761.47305218657, e: 0.096\n",
      "episode: 468/1000, score: 76254.20381263137, e: 0.096\n",
      "episode: 469/1000, score: 79310.96877339414, e: 0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 470/1000, score: 68692.28498336714, e: 0.095\n",
      "episode: 471/1000, score: 73944.12526040158, e: 0.094\n",
      "episode: 472/1000, score: 82276.82456694811, e: 0.094\n",
      "episode: 473/1000, score: 84760.53561844327, e: 0.093\n",
      "episode: 474/1000, score: 75622.21092970119, e: 0.093\n",
      "episode: 475/1000, score: 75460.45401766902, e: 0.092\n",
      "episode: 476/1000, score: 72074.80013398378, e: 0.092\n",
      "episode: 477/1000, score: 74524.32838277875, e: 0.092\n",
      "episode: 478/1000, score: 78579.55150883201, e: 0.091\n",
      "episode: 479/1000, score: 79207.11869957992, e: 0.091\n",
      "episode: 480/1000, score: 76396.59733313893, e: 0.09\n",
      "episode: 481/1000, score: 91009.68687028313, e: 0.09\n",
      "episode: 482/1000, score: 80972.40143935385, e: 0.089\n",
      "episode: 483/1000, score: 75229.38505682505, e: 0.089\n",
      "episode: 484/1000, score: 81341.30958655447, e: 0.088\n",
      "episode: 485/1000, score: 74968.08814365517, e: 0.088\n",
      "episode: 486/1000, score: 79934.56751885888, e: 0.088\n",
      "episode: 487/1000, score: 110055.92799593428, e: 0.087\n",
      "episode: 488/1000, score: 83436.82271135105, e: 0.087\n",
      "episode: 489/1000, score: 87443.0086903399, e: 0.086\n",
      "episode: 490/1000, score: 81181.84896484486, e: 0.086\n",
      "episode: 491/1000, score: 73227.16219871226, e: 0.085\n",
      "episode: 492/1000, score: 73644.48166289726, e: 0.085\n",
      "episode: 493/1000, score: 86163.79519061417, e: 0.084\n",
      "episode: 494/1000, score: 79526.99730911682, e: 0.084\n",
      "episode: 495/1000, score: 86838.45890601119, e: 0.084\n",
      "episode: 496/1000, score: 81505.94705661453, e: 0.083\n",
      "episode: 497/1000, score: 74704.79769416052, e: 0.083\n",
      "episode: 498/1000, score: 79975.64310448011, e: 0.082\n",
      "episode: 499/1000, score: 61866.666608433334, e: 0.082\n",
      "episode: 500/1000, score: 87816.44703577964, e: 0.082\n",
      "episode: 501/1000, score: 73233.39934328556, e: 0.081\n",
      "episode: 502/1000, score: 80885.40773330361, e: 0.081\n",
      "episode: 503/1000, score: 96938.30028218662, e: 0.08\n",
      "episode: 504/1000, score: 81861.58933093843, e: 0.08\n",
      "episode: 505/1000, score: 86830.17978713308, e: 0.08\n",
      "episode: 506/1000, score: 82730.4030413053, e: 0.079\n",
      "episode: 507/1000, score: 79659.48909074337, e: 0.079\n",
      "episode: 508/1000, score: 90556.02940290146, e: 0.078\n",
      "episode: 509/1000, score: 77551.70032893456, e: 0.078\n",
      "episode: 510/1000, score: 73878.57833094096, e: 0.078\n",
      "episode: 511/1000, score: 68275.2022533419, e: 0.077\n",
      "episode: 512/1000, score: 81034.33848145022, e: 0.077\n",
      "episode: 513/1000, score: 77251.08443492545, e: 0.076\n",
      "episode: 514/1000, score: 76262.15305276084, e: 0.076\n",
      "episode: 515/1000, score: 93734.91205165563, e: 0.076\n",
      "episode: 516/1000, score: 79119.0011008824, e: 0.075\n",
      "episode: 517/1000, score: 71875.34869331693, e: 0.075\n",
      "episode: 518/1000, score: 72058.90201459028, e: 0.075\n",
      "episode: 519/1000, score: 78261.30626364998, e: 0.074\n",
      "episode: 520/1000, score: 75575.88610567435, e: 0.074\n",
      "episode: 521/1000, score: 78851.09015762027, e: 0.073\n",
      "episode: 522/1000, score: 87059.4929677448, e: 0.073\n",
      "episode: 523/1000, score: 83424.86469096689, e: 0.073\n",
      "episode: 524/1000, score: 78487.44938874908, e: 0.072\n",
      "episode: 525/1000, score: 79310.87539786207, e: 0.072\n",
      "episode: 526/1000, score: 75690.77423761255, e: 0.072\n",
      "episode: 527/1000, score: 84105.87856354026, e: 0.071\n",
      "episode: 528/1000, score: 75887.66637626625, e: 0.071\n",
      "episode: 529/1000, score: 90346.54914391098, e: 0.071\n",
      "episode: 530/1000, score: 80001.0694877778, e: 0.07\n",
      "episode: 531/1000, score: 75221.72808269947, e: 0.07\n",
      "episode: 532/1000, score: 77065.98504687945, e: 0.069\n",
      "episode: 533/1000, score: 73112.55836154, e: 0.069\n",
      "episode: 534/1000, score: 86929.30204556644, e: 0.069\n"
     ]
    }
   ],
   "source": [
    "#Episodes to train\n",
    "for e in range(EPISODES):\n",
    "    \n",
    "    #Create initial state from time frame\n",
    "    state = env.reset()\n",
    "    #state = np.asarray(state)\n",
    "    #state = np.reshape(state, [1, 6, frame_size,])\n",
    "\n",
    "    #time frames\n",
    "    for time_frame in range(frame_size, 1000):\n",
    "        \n",
    "        #Agent takes action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        #Calc reward\n",
    "        next_state, reward, done = env.step(action)\n",
    "        #print(next_state)\n",
    "        #next_state = np.reshape(next_state, [1, 6, frame_size])\n",
    "        \n",
    "        #Remember action\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        #Override state with next state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, env.get_score(), agent.epsilon))\n",
    "            break\n",
    "       \n",
    "    #Replay\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "    # if e % 10 == 0:\n",
    "    #     agent.save(\"./save/cartpole-dqn.h5\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calc_next_state(state):\n",
    "#Calc Capital (action == 0 then sell, action == 1 then buy) 5 coins * state_close_price\n",
    "        #Calc position (current position -/+ differ from action)\n",
    "        #Get newest state item and append capital and position\n",
    "        #Remove latest entry and add new state item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing parameters\n",
    "TEST_EPISODES = 10\n",
    "TEST_CAPITAL = 100000\n",
    "TEST_DATABASE = DB[\"etherium_usd\"]\n",
    "TEST_FRAME_SIZE = 100\n",
    "TEST_BUY_STEP_SIZE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Environment\n",
    "test_env = TradingGame(CAPITAL, TEST_FRAME_SIZE, TEST_BUY_STEP_SIZE, TEST_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Episodes to train\n",
    "for e in range(TEST_EPISODES):\n",
    "    \n",
    "    #Create initial state from time frame\n",
    "    state = test_env.reset()\n",
    "\n",
    "    #time frames\n",
    "    for time_frame in range(frame_size, 1000):\n",
    "        \n",
    "        #Agent takes action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        #Calc reward\n",
    "        next_state, reward, done = test_env.step(action)\n",
    "        \n",
    "        #Override state with next state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, env.get_score(), agent.epsilon))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
