{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Trading Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitcoin_usd\n",
      "etherium_usd\n",
      "ripple_usd\n"
     ]
    }
   ],
   "source": [
    "#TO READ THE DATABASE\n",
    "import h5py\n",
    "\n",
    "DB_FILE = \"../data/dataset_1h_1000.hdf5\"\n",
    "DB = h5py.File(DB_FILE, \"r\")\n",
    "\n",
    "for item in DB:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.51543080e+12   1.47930000e+04   1.46940000e+04   1.48600000e+04\n",
      "    1.46690000e+04   7.86847895e+02]\n",
      " [  1.51542720e+12   1.44000000e+04   1.47890000e+04   1.49230000e+04\n",
      "    1.43470000e+04   4.20724394e+03]]\n"
     ]
    }
   ],
   "source": [
    "# Show the values of the first timestep\n",
    "print(DB[\"bitcoin_usd\"][0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Dense, Flatten, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, frame_size, action_size):\n",
    "        self.frame_size = frame_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        inputs = Input(shape=(6,100))\n",
    "        x = Flatten()(inputs)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(16, activation='relu')(x)\n",
    "        out = Dense(self.action_size, activation='linear')(x)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=out)\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        print(model.summary())\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # The agent acts randomly\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        # Predict the reward value based on the given state\n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        # Pick the action based on the predicted reward\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # Sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # Extract informations from each memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            # if done, make our target reward\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # predict the future discounted reward\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "                \n",
    "            # make the agent to approximately map\n",
    "            # the current state to future discounted reward\n",
    "            # We'll call that target_f\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            # Train the Neural Net with the state and target_f\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trading Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TradingGame:\n",
    "    def __init__(self, start_capital, frame_size, database):\n",
    "        self.start_capital = start_capital\n",
    "        self.frame_size = frame_size\n",
    "        \n",
    "        self.database = np.asarray(database[\"bitcoin_usd\"])\n",
    "        self.capital = start_capital\n",
    "        self.timestep = 0\n",
    "        self.time_frame = self.database[0:self.frame_size]\n",
    "        \n",
    "        self.position = 0\n",
    "        self.state = None\n",
    "    \n",
    "    ### Reset function\n",
    "    def reset(self):\n",
    "        # Set everything to the initial start value\n",
    "        self.capital = self.start_capital\n",
    "        self.timestep = 0\n",
    "        self.time_frame = self.database[0:self.frame_size]\n",
    "        \n",
    "        self.position = 0\n",
    "        \n",
    "        # state => (time_frame, capital, position)\n",
    "        self.state = self.time_frame #self.capital, self.position)\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    ### Next frame function\n",
    "    def get_next_frame(self):\n",
    "        self.timestep += 1\n",
    "        \n",
    "        if (self.frame_size+self.timestep) < self.database.shape[0]:\n",
    "            next_frame = self.database[self.timestep:self.frame_size+self.timestep]\n",
    "            done = False\n",
    "        else:\n",
    "            next_frame = self.time_frame\n",
    "            done = True\n",
    "            \n",
    "        return next_frame, done\n",
    "    \n",
    "    ### Reward function\n",
    "    def calc_reward(self, action, next_frame):\n",
    "        # Get new price\n",
    "        new_price = next_frame[self.frame_size-1, 2]\n",
    "        \n",
    "        # Check the action and calculate the reward\n",
    "        #hold\n",
    "        if action == 0: \n",
    "            reward = 0\n",
    "            \n",
    "        #buy\n",
    "        elif action == 1: \n",
    "            if self.position is 0:\n",
    "                reward = 0\n",
    "                self.position = new_price\n",
    "            else:\n",
    "                reward = -10\n",
    "                \n",
    "        #sell\n",
    "        elif action == 2: \n",
    "            if self.position is not 0:\n",
    "                reward = new_price - self.position\n",
    "                self.position = 0\n",
    "            else:\n",
    "                reward = -10\n",
    "        return reward\n",
    "    \n",
    "    ### Next step function\n",
    "    def step(self, action):\n",
    "        # Get next time frame\n",
    "        next_frame, done = self.get_next_frame()\n",
    "        \n",
    "        # Get reward\n",
    "        reward = self.calc_reward(action, next_frame)\n",
    "        \n",
    "        # check if done\n",
    "        self.capital += reward\n",
    "        if self.capital <= 0:\n",
    "            done = True\n",
    "            \n",
    "        # Next state\n",
    "        next_state = next_frame #self.capital, self.position)\n",
    "        \n",
    "        # update state\n",
    "        self.state = next_state\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameter for Agent\n",
    "frame_size = 100 # Time frame\n",
    "#state_attributes = 8\n",
    "action_size = 3 # hold, buy, sell\n",
    "\n",
    "# Parameter for Trading\n",
    "batch_size = 128\n",
    "EPISODES = 1000\n",
    "CAPITAL = 1000\n",
    "#POSITION = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 6, 100)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                19232     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 51        \n",
      "=================================================================\n",
      "Total params: 20,867\n",
      "Trainable params: 20,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create agent\n",
    "agent = DQNAgent(frame_size, action_size)\n",
    "\n",
    "# Initialize Environment\n",
    "env = TradingGame(CAPITAL, frame_size, DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/1000, score: -1.097828980000486, e: 1.0\n",
      "episode: 1/1000, score: -1.9511952700013353, e: 1.0\n",
      "episode: 2/1000, score: -11.203802529998939, e: 0.99\n",
      "episode: 3/1000, score: -247.14101024999945, e: 0.99\n",
      "episode: 4/1000, score: -45.097828980000486, e: 0.99\n",
      "episode: 5/1000, score: -117.95889014000022, e: 0.98\n",
      "episode: 6/1000, score: -1261.0978289800005, e: 0.98\n",
      "episode: 7/1000, score: -21.902171019999514, e: 0.97\n",
      "episode: 8/1000, score: 4025.887837389997, e: 0.97\n",
      "episode: 9/1000, score: 9062.94342805, e: 0.96\n",
      "episode: 10/1000, score: -503.2352294499997, e: 0.96\n",
      "episode: 11/1000, score: -127.90217101999951, e: 0.95\n",
      "episode: 12/1000, score: -265.3016315099994, e: 0.95\n",
      "episode: 13/1000, score: -205.60402869000063, e: 0.94\n",
      "episode: 14/1000, score: -271.0, e: 0.94\n",
      "episode: 15/1000, score: -52.018728420001935, e: 0.93\n",
      "episode: 16/1000, score: -152.23883922999994, e: 0.93\n",
      "episode: 17/1000, score: -1514.198071750001, e: 0.92\n",
      "episode: 18/1000, score: -320.0, e: 0.92\n",
      "episode: 19/1000, score: -255.0, e: 0.91\n",
      "episode: 20/1000, score: -108.09782898000049, e: 0.91\n",
      "episode: 21/1000, score: -57.0, e: 0.9\n",
      "episode: 22/1000, score: -454.4405703499997, e: 0.9\n",
      "episode: 23/1000, score: -13.0, e: 0.9\n",
      "episode: 24/1000, score: -323.6500497800007, e: 0.89\n",
      "episode: 25/1000, score: -210.31079759000204, e: 0.89\n",
      "episode: 26/1000, score: -118.9838751999996, e: 0.88\n",
      "episode: 27/1000, score: -84.12011859000086, e: 0.88\n",
      "episode: 28/1000, score: -188.5594296500003, e: 0.87\n",
      "episode: 29/1000, score: -325.8237690900005, e: 0.87\n",
      "episode: 30/1000, score: -745.1829542000014, e: 0.86\n",
      "episode: 31/1000, score: -386.6572586300008, e: 0.86\n",
      "episode: 32/1000, score: -910.7550876100013, e: 0.86\n",
      "episode: 33/1000, score: -73.56063095000172, e: 0.85\n",
      "episode: 34/1000, score: -88.0, e: 0.85\n",
      "episode: 35/1000, score: -731.3016315099994, e: 0.84\n",
      "episode: 36/1000, score: -67.75508761000128, e: 0.84\n",
      "episode: 37/1000, score: -55.0, e: 0.83\n",
      "episode: 38/1000, score: -4.2595890399989, e: 0.83\n",
      "episode: 39/1000, score: -783.7961974700011, e: 0.83\n",
      "episode: 40/1000, score: -117.0, e: 0.82\n",
      "episode: 41/1000, score: -0.6983684900005755, e: 0.82\n",
      "episode: 42/1000, score: -526.5502118900022, e: 0.81\n",
      "episode: 43/1000, score: -81.0, e: 0.81\n",
      "episode: 44/1000, score: -52.95889014000022, e: 0.81\n",
      "episode: 45/1000, score: -229.6572586300008, e: 0.8\n",
      "episode: 46/1000, score: -204.6572586300008, e: 0.8\n",
      "episode: 47/1000, score: -867.4337407000003, e: 0.79\n",
      "episode: 48/1000, score: -454.0, e: 0.79\n",
      "episode: 49/1000, score: 5922.731806529995, e: 0.79\n",
      "episode: 50/1000, score: -222.01354649999848, e: 0.78\n",
      "episode: 51/1000, score: -31.698368490000576, e: 0.78\n",
      "episode: 52/1000, score: -74.30163150999942, e: 0.77\n",
      "episode: 53/1000, score: -7.958890140000221, e: 0.77\n",
      "episode: 54/1000, score: -150.03673770000205, e: 0.77\n",
      "episode: 55/1000, score: -202.3427413699992, e: 0.76\n",
      "episode: 56/1000, score: -178.22651511000004, e: 0.76\n",
      "episode: 57/1000, score: -8.440570349999689, e: 0.76\n",
      "episode: 58/1000, score: -90.0, e: 0.75\n",
      "episode: 59/1000, score: -111.53758390000075, e: 0.75\n",
      "episode: 60/1000, score: -158.0, e: 0.74\n",
      "episode: 61/1000, score: -342.0, e: 0.74\n",
      "episode: 62/1000, score: -106.21296861000155, e: 0.74\n",
      "episode: 63/1000, score: -243.30163150999942, e: 0.73\n",
      "episode: 64/1000, score: -366.6443728799986, e: 0.73\n",
      "episode: 65/1000, score: -132.0, e: 0.73\n",
      "episode: 66/1000, score: -164.51624838000134, e: 0.72\n",
      "episode: 67/1000, score: -8.0, e: 0.72\n",
      "episode: 68/1000, score: -94.3427413699992, e: 0.71\n",
      "episode: 69/1000, score: -447.0, e: 0.71\n",
      "episode: 70/1000, score: -78.26762496999982, e: 0.71\n",
      "episode: 71/1000, score: -127.95889014000022, e: 0.7\n",
      "episode: 72/1000, score: -223.34646103999876, e: 0.7\n",
      "episode: 73/1000, score: 2990.2548299299997, e: 0.7\n",
      "episode: 74/1000, score: -63.0, e: 0.69\n",
      "episode: 75/1000, score: -384.43074352000076, e: 0.69\n",
      "episode: 76/1000, score: 1118.3152478600005, e: 0.69\n",
      "episode: 77/1000, score: -255.0978289800005, e: 0.68\n",
      "episode: 78/1000, score: -190.0, e: 0.68\n",
      "episode: 79/1000, score: -825.0, e: 0.68\n",
      "episode: 80/1000, score: -368.6983684900006, e: 0.67\n",
      "episode: 81/1000, score: -714.0, e: 0.67\n",
      "episode: 82/1000, score: -64.0, e: 0.67\n",
      "episode: 83/1000, score: -990.0, e: 0.66\n",
      "episode: 84/1000, score: -1371.5692564799992, e: 0.66\n",
      "episode: 85/1000, score: -210.0, e: 0.66\n",
      "episode: 86/1000, score: -695.6983684900006, e: 0.65\n",
      "episode: 87/1000, score: -189.0, e: 0.65\n",
      "episode: 88/1000, score: -258.0, e: 0.65\n",
      "episode: 89/1000, score: -717.9551704700007, e: 0.64\n",
      "episode: 90/1000, score: -391.6572586300008, e: 0.64\n",
      "episode: 91/1000, score: -190.9021710199995, e: 0.64\n",
      "episode: 92/1000, score: -112.30163150999942, e: 0.63\n",
      "episode: 93/1000, score: -342.0, e: 0.63\n",
      "episode: 94/1000, score: -634.0, e: 0.63\n",
      "episode: 95/1000, score: -462.3016315099994, e: 0.62\n",
      "episode: 96/1000, score: -497.0, e: 0.62\n",
      "episode: 97/1000, score: -1087.7865972499985, e: 0.62\n",
      "episode: 98/1000, score: -224.3427413699992, e: 0.61\n",
      "episode: 99/1000, score: -234.0, e: 0.61\n",
      "episode: 100/1000, score: -129.78265923000072, e: 0.61\n",
      "episode: 101/1000, score: -183.0, e: 0.61\n",
      "episode: 102/1000, score: -1.1566220599997905, e: 0.6\n",
      "episode: 103/1000, score: -474.6912459599989, e: 0.6\n",
      "episode: 104/1000, score: -7.752229900002931, e: 0.6\n",
      "episode: 105/1000, score: -301.3275021299978, e: 0.59\n",
      "episode: 106/1000, score: -30.758019090000744, e: 0.59\n",
      "episode: 107/1000, score: -187.0, e: 0.59\n",
      "episode: 108/1000, score: -101.30163150999942, e: 0.58\n",
      "episode: 109/1000, score: -2.5557099800007563, e: 0.58\n",
      "episode: 110/1000, score: -356.06405043999985, e: 0.58\n",
      "episode: 111/1000, score: -678.0, e: 0.58\n",
      "episode: 112/1000, score: -265.0, e: 0.57\n",
      "episode: 113/1000, score: -17.657258630000797, e: 0.57\n",
      "episode: 114/1000, score: -113.43074352000076, e: 0.57\n",
      "episode: 115/1000, score: -728.0, e: 0.56\n",
      "episode: 116/1000, score: -220.0978289800005, e: 0.56\n",
      "episode: 117/1000, score: -30.0, e: 0.56\n",
      "episode: 118/1000, score: -762.0978289800005, e: 0.56\n",
      "episode: 119/1000, score: -91.55942965000031, e: 0.55\n",
      "episode: 120/1000, score: -248.87801052000032, e: 0.55\n",
      "episode: 121/1000, score: -21.0, e: 0.55\n",
      "episode: 122/1000, score: -3.528580759999386, e: 0.55\n",
      "episode: 123/1000, score: -203.0, e: 0.54\n",
      "episode: 124/1000, score: -85.6572586300008, e: 0.54\n",
      "episode: 125/1000, score: -920.1254406699991, e: 0.54\n",
      "episode: 126/1000, score: -236.9021710199995, e: 0.53\n",
      "episode: 127/1000, score: -439.0, e: 0.53\n",
      "episode: 128/1000, score: -150.0978289800005, e: 0.53\n",
      "episode: 129/1000, score: -520.0, e: 0.53\n",
      "episode: 130/1000, score: -154.76116077000006, e: 0.52\n",
      "episode: 131/1000, score: -1.0, e: 0.52\n",
      "episode: 132/1000, score: -256.1426585099998, e: 0.52\n",
      "episode: 133/1000, score: -2.1776385200009827, e: 0.52\n",
      "episode: 134/1000, score: -106.90217101999951, e: 0.51\n",
      "episode: 135/1000, score: -621.0, e: 0.51\n",
      "episode: 136/1000, score: -214.0, e: 0.51\n",
      "episode: 137/1000, score: -381.9685997400011, e: 0.51\n",
      "episode: 138/1000, score: -492.6983684900006, e: 0.5\n",
      "episode: 139/1000, score: -547.123068339999, e: 0.5\n",
      "episode: 140/1000, score: -125.50336262999917, e: 0.5\n",
      "episode: 141/1000, score: -120.56925647999924, e: 0.5\n",
      "episode: 142/1000, score: -169.8233825899988, e: 0.49\n",
      "episode: 143/1000, score: -547.8634204099999, e: 0.49\n",
      "episode: 144/1000, score: -174.76116077000006, e: 0.49\n",
      "episode: 145/1000, score: -357.11033400999986, e: 0.49\n",
      "episode: 146/1000, score: -432.0, e: 0.48\n",
      "episode: 147/1000, score: -536.0, e: 0.48\n",
      "episode: 148/1000, score: -37.0, e: 0.48\n",
      "episode: 149/1000, score: -65.0, e: 0.48\n",
      "episode: 150/1000, score: -564.0, e: 0.47\n",
      "episode: 151/1000, score: -385.0, e: 0.47\n",
      "episode: 152/1000, score: -782.0, e: 0.47\n",
      "episode: 153/1000, score: -146.0959093500005, e: 0.47\n",
      "episode: 154/1000, score: -679.3427413699992, e: 0.46\n",
      "episode: 155/1000, score: -554.3016315099994, e: 0.46\n",
      "episode: 156/1000, score: -229.3427413699992, e: 0.46\n",
      "episode: 157/1000, score: -322.5717537700002, e: 0.46\n",
      "episode: 158/1000, score: -386.4375054100019, e: 0.46\n",
      "episode: 159/1000, score: -937.0, e: 0.45\n",
      "episode: 160/1000, score: -293.7826592300007, e: 0.45\n",
      "episode: 161/1000, score: -15.0, e: 0.45\n",
      "episode: 162/1000, score: -627.1410102499995, e: 0.45\n",
      "episode: 163/1000, score: -281.43074352000076, e: 0.44\n",
      "episode: 164/1000, score: -129.0, e: 0.44\n",
      "episode: 165/1000, score: -145.0, e: 0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 166/1000, score: -437.0, e: 0.44\n",
      "episode: 167/1000, score: -172.0, e: 0.44\n",
      "episode: 168/1000, score: -4.0, e: 0.43\n",
      "episode: 169/1000, score: -132.0, e: 0.43\n",
      "episode: 170/1000, score: -47.80759975999899, e: 0.43\n",
      "episode: 171/1000, score: -27.0, e: 0.43\n",
      "episode: 172/1000, score: -905.0, e: 0.42\n",
      "episode: 173/1000, score: -532.9476739500005, e: 0.42\n",
      "episode: 174/1000, score: -5.0, e: 0.42\n",
      "episode: 175/1000, score: -168.609881219998, e: 0.42\n",
      "episode: 176/1000, score: -1431.5450504799992, e: 0.42\n",
      "episode: 177/1000, score: -477.0, e: 0.41\n",
      "episode: 178/1000, score: 3397.160477130001, e: 0.41\n",
      "episode: 179/1000, score: -342.9021710199995, e: 0.41\n",
      "episode: 180/1000, score: -526.8708424899978, e: 0.41\n",
      "episode: 181/1000, score: 3722.6805673399977, e: 0.41\n",
      "episode: 182/1000, score: -56.433678090001195, e: 0.4\n",
      "episode: 183/1000, score: 3107.6241121899966, e: 0.4\n",
      "episode: 184/1000, score: -167.10154865000004, e: 0.4\n",
      "episode: 185/1000, score: -686.0, e: 0.4\n",
      "episode: 186/1000, score: -346.6572586300008, e: 0.4\n",
      "episode: 187/1000, score: -1148.1410102499995, e: 0.39\n",
      "episode: 188/1000, score: -59.398111570000765, e: 0.39\n",
      "episode: 189/1000, score: 4569.285583230001, e: 0.39\n",
      "episode: 190/1000, score: -76.0, e: 0.39\n",
      "episode: 191/1000, score: -133.0, e: 0.39\n",
      "episode: 192/1000, score: -1151.742201859999, e: 0.38\n",
      "episode: 193/1000, score: -706.0, e: 0.38\n",
      "episode: 194/1000, score: -167.0, e: 0.38\n",
      "episode: 195/1000, score: 5206.431485519999, e: 0.38\n",
      "episode: 196/1000, score: -758.0, e: 0.38\n",
      "episode: 197/1000, score: -318.7550876100013, e: 0.37\n",
      "episode: 198/1000, score: -63.0, e: 0.37\n",
      "episode: 199/1000, score: -457.6983684900006, e: 0.37\n",
      "episode: 200/1000, score: -281.90566020000006, e: 0.37\n",
      "episode: 201/1000, score: -419.0, e: 0.37\n",
      "episode: 202/1000, score: -208.6005395100001, e: 0.37\n",
      "episode: 203/1000, score: -302.0, e: 0.36\n",
      "episode: 204/1000, score: -58.3427413699992, e: 0.36\n",
      "episode: 205/1000, score: -12.0, e: 0.36\n",
      "episode: 206/1000, score: -342.0, e: 0.36\n",
      "episode: 207/1000, score: -262.0, e: 0.36\n",
      "episode: 208/1000, score: -893.7982688800003, e: 0.35\n",
      "episode: 209/1000, score: -59.48905696000111, e: 0.35\n",
      "episode: 210/1000, score: -788.6572586300008, e: 0.35\n",
      "episode: 211/1000, score: -443.6983684900006, e: 0.35\n",
      "episode: 212/1000, score: -1217.4747360499987, e: 0.35\n",
      "episode: 213/1000, score: -309.0, e: 0.35\n",
      "episode: 214/1000, score: 519.1493172500013, e: 0.34\n",
      "episode: 215/1000, score: 2320.816208020002, e: 0.34\n",
      "episode: 216/1000, score: -274.3016315099994, e: 0.34\n",
      "episode: 217/1000, score: -1082.3427413699992, e: 0.34\n",
      "episode: 218/1000, score: -8.0, e: 0.34\n",
      "episode: 219/1000, score: -30.0, e: 0.34\n",
      "episode: 220/1000, score: -123.0, e: 0.33\n",
      "episode: 221/1000, score: -598.0, e: 0.33\n",
      "episode: 222/1000, score: -511.0, e: 0.33\n",
      "episode: 223/1000, score: -581.77305075, e: 0.33\n",
      "episode: 224/1000, score: -255.0, e: 0.33\n",
      "episode: 225/1000, score: -335.0, e: 0.33\n",
      "episode: 226/1000, score: -150.0, e: 0.32\n",
      "episode: 227/1000, score: -597.0, e: 0.32\n",
      "episode: 228/1000, score: -603.5692564799992, e: 0.32\n",
      "episode: 229/1000, score: 5753.456245469999, e: 0.32\n",
      "episode: 230/1000, score: 3606.093965260001, e: 0.32\n",
      "episode: 231/1000, score: -505.3016315099994, e: 0.32\n",
      "episode: 232/1000, score: -315.56925647999924, e: 0.31\n",
      "episode: 233/1000, score: -647.6572586300008, e: 0.31\n",
      "episode: 234/1000, score: -792.0, e: 0.31\n",
      "episode: 235/1000, score: -518.9021710199995, e: 0.31\n",
      "episode: 236/1000, score: -173.72857285999999, e: 0.31\n",
      "episode: 237/1000, score: -66.0, e: 0.31\n",
      "episode: 238/1000, score: -1376.0978289800005, e: 0.3\n",
      "episode: 239/1000, score: -489.0, e: 0.3\n",
      "episode: 240/1000, score: -154.0, e: 0.3\n",
      "episode: 241/1000, score: -827.2374903100008, e: 0.3\n",
      "episode: 242/1000, score: -355.0978289800005, e: 0.3\n",
      "episode: 243/1000, score: -202.21296861000155, e: 0.3\n",
      "episode: 244/1000, score: -208.0, e: 0.3\n",
      "episode: 245/1000, score: -447.0, e: 0.29\n",
      "episode: 246/1000, score: -40.0, e: 0.29\n",
      "episode: 247/1000, score: 2675.310746950001, e: 0.29\n",
      "episode: 248/1000, score: -732.0, e: 0.29\n",
      "episode: 249/1000, score: -138.3427413699992, e: 0.29\n",
      "episode: 250/1000, score: -288.0, e: 0.29\n",
      "episode: 251/1000, score: -114.0, e: 0.29\n",
      "episode: 252/1000, score: -335.0, e: 0.28\n",
      "episode: 253/1000, score: -205.0, e: 0.28\n",
      "episode: 254/1000, score: -1110.6080360300002, e: 0.28\n",
      "episode: 255/1000, score: 123.75883124000029, e: 0.28\n",
      "episode: 256/1000, score: -797.0, e: 0.28\n",
      "episode: 257/1000, score: -1484.0, e: 0.28\n",
      "episode: 258/1000, score: -258.0, e: 0.28\n",
      "episode: 259/1000, score: -141.72609826000007, e: 0.27\n",
      "episode: 260/1000, score: -601.0, e: 0.27\n",
      "episode: 261/1000, score: -352.55570998000076, e: 0.27\n",
      "episode: 262/1000, score: -81.30163150999942, e: 0.27\n",
      "episode: 263/1000, score: 1089.3332559299997, e: 0.27\n",
      "episode: 264/1000, score: -957.5557099800008, e: 0.27\n",
      "episode: 265/1000, score: -28.0, e: 0.27\n",
      "episode: 266/1000, score: 1109.1373730999985, e: 0.26\n",
      "episode: 267/1000, score: -988.0, e: 0.26\n",
      "episode: 268/1000, score: 1180.250265570001, e: 0.26\n",
      "episode: 269/1000, score: 285.59999999999854, e: 0.26\n",
      "episode: 270/1000, score: -73.0, e: 0.26\n",
      "episode: 271/1000, score: -1078.0, e: 0.26\n",
      "episode: 272/1000, score: -58.444290019999244, e: 0.26\n",
      "episode: 273/1000, score: -30.0, e: 0.26\n",
      "episode: 274/1000, score: -1066.0, e: 0.25\n",
      "episode: 275/1000, score: -627.0, e: 0.25\n",
      "episode: 276/1000, score: -405.0, e: 0.25\n",
      "episode: 277/1000, score: -222.03600856000048, e: 0.25\n",
      "episode: 278/1000, score: -238.14101024999945, e: 0.25\n",
      "episode: 279/1000, score: -372.0, e: 0.25\n",
      "episode: 280/1000, score: -300.6967202300002, e: 0.25\n",
      "episode: 281/1000, score: -1323.9021710199995, e: 0.25\n",
      "episode: 282/1000, score: -197.3427413699992, e: 0.24\n",
      "episode: 283/1000, score: -32.0, e: 0.24\n",
      "episode: 284/1000, score: -539.0, e: 0.24\n",
      "episode: 285/1000, score: -168.0, e: 0.24\n",
      "episode: 286/1000, score: -467.0, e: 0.24\n",
      "episode: 287/1000, score: -477.55570998000076, e: 0.24\n",
      "episode: 288/1000, score: -574.9464610399991, e: 0.24\n",
      "episode: 289/1000, score: -1197.0, e: 0.24\n",
      "episode: 290/1000, score: -215.95889014000022, e: 0.23\n",
      "episode: 291/1000, score: -436.6983684900006, e: 0.23\n",
      "episode: 292/1000, score: -584.0, e: 0.23\n",
      "episode: 293/1000, score: -95.0, e: 0.23\n",
      "episode: 294/1000, score: 2043.5562948100014, e: 0.23\n",
      "episode: 295/1000, score: -735.0, e: 0.23\n",
      "episode: 296/1000, score: -501.0978289800005, e: 0.23\n",
      "episode: 297/1000, score: -1.0, e: 0.23\n",
      "episode: 298/1000, score: 1619.1705119500002, e: 0.23\n",
      "episode: 299/1000, score: -636.0, e: 0.22\n",
      "episode: 300/1000, score: -399.6983684900006, e: 0.22\n",
      "episode: 301/1000, score: 1419.0603857199985, e: 0.22\n",
      "episode: 302/1000, score: -972.4608781799998, e: 0.22\n",
      "episode: 303/1000, score: 2218.6576829999995, e: 0.22\n",
      "episode: 304/1000, score: -532.8311381900003, e: 0.22\n",
      "episode: 305/1000, score: -395.0, e: 0.22\n",
      "episode: 306/1000, score: -517.0, e: 0.22\n",
      "episode: 307/1000, score: -206.94074316000115, e: 0.22\n",
      "episode: 308/1000, score: 1415.0, e: 0.21\n",
      "episode: 309/1000, score: -128.05872557000112, e: 0.21\n",
      "episode: 310/1000, score: -1870.9021710199995, e: 0.21\n",
      "episode: 311/1000, score: -106.43074352000076, e: 0.21\n",
      "episode: 312/1000, score: -798.0, e: 0.21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e12676cd255c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#Agent takes action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#Calc reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-2f7363b3c2cf>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Predict the reward value based on the given state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mact_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Pick the action based on the predicted reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1573\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1201\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rens/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Episodes to train\n",
    "for e in range(EPISODES):\n",
    "    \n",
    "    #Create initial state from time frame\n",
    "    state = env.reset()\n",
    "    state = np.asarray(state)\n",
    "    state = np.reshape(state, [1, 6, frame_size,])\n",
    "\n",
    "    #time frames\n",
    "    for time_frame in range(frame_size, 1000):\n",
    "        \n",
    "        #Agent takes action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        #Calc reward\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, 6, frame_size])\n",
    "        \n",
    "        #Remember action\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        #Override state with next state\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "                  .format(e, EPISODES, env.get_score(), agent.epsilon))\n",
    "            break\n",
    "       \n",
    "    #Replay\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "    # if e % 10 == 0:\n",
    "    #     agent.save(\"./save/cartpole-dqn.h5\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def calc_next_state(state):\n",
    "#Calc Capital (action == 0 then sell, action == 1 then buy) 5 coins * state_close_price\n",
    "        #Calc position (current position -/+ differ from action)\n",
    "        #Get newest state item and append capital and position\n",
    "        #Remove latest entry and add new state item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
